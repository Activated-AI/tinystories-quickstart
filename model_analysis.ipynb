{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block_size: 512\n",
      "n_layer: 14\n",
      "n_head: 16\n",
      "n_embd: 512\n",
      "feed_forward_factor: 2.5\n",
      "vocab_size: 8192\n",
      "data_dir: dataset\n",
      "expt_name: restart_good_3hr_search\n",
      "batch_size: 128\n",
      "max_lr: 0.002\n",
      "min_lr: 0.0001\n",
      "beta_1: 0.9\n",
      "beta_2: 0.99\n",
      "warmup_steps: 50\n",
      "max_steps: 60000\n",
      "max_runtime_seconds: 10800\n",
      "weight_decay: 0.12\n",
      "need_epoch_reshuffle: True\n",
      "matmul_precision: high\n",
      "smoke_test: False\n"
     ]
    }
   ],
   "source": [
    "from gpt import GPTConfig, GPT, generate\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lucky_diego_1                                      0.364 step 3750 | val loss 1.4910 | byte loss 0.3645 | ds 691.5s\n",
      "16l_16h_512d_hour_good_max                         0.366 step 6250 | val loss 1.4960 | byte loss 0.3657 | ds 708.9s\n",
      "16l_16h_512d_hour                                  0.369 step 6250 | val loss 1.5093 | byte loss 0.3690 | ds 708.7s\n",
      "diego_is_12m_man                                   0.37 step 4299 | val loss 1.5155 | byte loss 0.3705 | ds 376.3s\n",
      "16l_16h_512d_3_hour_good_max_hig_prec              0.375 step 4000 | val loss 1.5341 | byte loss 0.3750 | ds 686.7s\n",
      "16l_16h_512d_3_hour_good_max_med_prec_fixed_shuffle 0.376 step 4000 | val loss 1.5363 | byte loss 0.3756 | ds 687.0s\n",
      "lucky_diego_2                                      0.378 step 3750 | val loss 1.5461 | byte loss 0.3780 | ds 691.5s\n",
      "lucky_diego_5                                      0.379 step 3750 | val loss 1.5490 | byte loss 0.3787 | ds 693.7s\n",
      "lucky_diego_6                                      0.379 step 3750 | val loss 1.5497 | byte loss 0.3788 | ds 691.9s\n",
      "16l_16h_512d_8_hour_good_max                       0.38 step 6250 | val loss 1.5538 | byte loss 0.3798 | ds 710.3s\n"
     ]
    }
   ],
   "source": [
    "def get_value(key, line, default):\n",
    "    try:\n",
    "        str_val = line.split(key)[1].split(' ')[1]\n",
    "        str_val = str_val.replace('s', '')\n",
    "        return float(str_val)\n",
    "    except:\n",
    "        return default\n",
    "\n",
    "def print_leaderboard_and_get_best_model(time_limit = None):\n",
    "    candidates = []\n",
    "    for expt in os.listdir('logs'):        \n",
    "        if 'smoke' in expt:\n",
    "            continue\n",
    "        best_loss_for_expt = 10000\n",
    "        best_line = ''\n",
    "\n",
    "        for line in open(f'logs/{expt}/log.txt').readlines():\n",
    "            if 'val' in line and 'byte loss' in line:\n",
    "                if time_limit is not None:\n",
    "                    time = get_value('ds', line, 100000)\n",
    "                    if time > time_limit:\n",
    "                        continue\n",
    "\n",
    "                byte_loss = get_value('byte loss', line, 100000)\n",
    "\n",
    "                if byte_loss < best_loss_for_expt:\n",
    "                    best_loss_for_expt = byte_loss\n",
    "                    best_line = line\n",
    "            \n",
    "        \n",
    "        if best_loss_for_expt < 10000:\n",
    "            candidates.append((best_loss_for_expt, expt, best_line))\n",
    "\n",
    "    candidates = sorted(candidates, key=lambda x: x[0])\n",
    "    for best_loss, expt, line in candidates:\n",
    "        print(expt.ljust(50), f'{best_loss:.3} {line.strip()}')\n",
    "\n",
    "    best_expt = candidates[0][1]\n",
    "    best_model_paths = [model_path for model_path in os.listdir(f'logs/{best_expt}') if 'model' in model_path]\n",
    "    best_model_path = sorted(best_model_paths, key=lambda x: x.split('_')[1])[-1]\n",
    "    return best_expt, best_model_path\n",
    "\n",
    "best_expt, best_model_path = print_leaderboard_and_get_best_model(time_limit=720)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lucky_diego_1                                      0.364 step 3750 | val loss 1.4910 | byte loss 0.3645 | ds 691.5s\n",
      "16l_16h_512d_hour_good_max                         0.366 step 6250 | val loss 1.4960 | byte loss 0.3657 | ds 708.9s\n",
      "16l_16h_512d_hour                                  0.369 step 6250 | val loss 1.5093 | byte loss 0.3690 | ds 708.7s\n",
      "16l_16h_512d_3_hour_good_max_hig_prec              0.375 step 4000 | val loss 1.5341 | byte loss 0.3750 | ds 686.7s\n",
      "16l_16h_512d_3_hour_good_max_med_prec_fixed_shuffle 0.376 step 4000 | val loss 1.5363 | byte loss 0.3756 | ds 687.0s\n",
      "lucky_diego_2                                      0.378 step 3750 | val loss 1.5461 | byte loss 0.3780 | ds 691.5s\n",
      "lucky_diego_5                                      0.379 step 3750 | val loss 1.5490 | byte loss 0.3787 | ds 693.7s\n",
      "lucky_diego_6                                      0.379 step 3750 | val loss 1.5497 | byte loss 0.3788 | ds 691.9s\n",
      "16l_16h_512d_8_hour_good_max                       0.38 step 6250 | val loss 1.5538 | byte loss 0.3798 | ds 710.3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('lucky_diego_1', 'model_06399.pt')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this sucks, make time limit a parameter to the existing print_leaderboard_and_get_best_model func\n",
    "\n",
    "\n",
    "def print_leaderboard_and_get_best_model_fast():\n",
    "    candidates = []\n",
    "    for expt in os.listdir('logs'):        \n",
    "        if 'smoke' in expt:\n",
    "            continue\n",
    "        best_loss_for_expt = 10000\n",
    "        best_line = ''\n",
    "\n",
    "        for line in open(f'logs/{expt}/log.txt').readlines():\n",
    "            if 'val' in line and 'byte loss' in line\n",
    "\n",
    "                def get_byte_loss():\n",
    "                    after_bl_str = line.split('byte loss')[1]            \n",
    "                    try:                \n",
    "                        byte_loss = float(after_bl_str.split(' ')[1])\n",
    "                    except ValueError as e:\n",
    "                        byte_loss = 100000\n",
    "                    return byte_loss\n",
    "                \n",
    "                byte_loss = get_byte_loss()\n",
    "\n",
    "                if byte_loss < best_loss_for_expt:\n",
    "            \n",
    "                    best_loss_for_expt = byte_loss\n",
    "                    best_line = line\n",
    "            \n",
    "        \n",
    "        if best_loss_for_expt < 10000:\n",
    "            candidates.append((best_loss_for_expt, expt, best_line))\n",
    "\n",
    "    candidates = sorted(candidates, key=lambda x: x[0])\n",
    "    for best_loss, expt, line in candidates:\n",
    "        print(expt.ljust(50), f'{best_loss:.3} {line.strip()}')\n",
    "\n",
    "    best_expt = candidates[0][1]\n",
    "    best_model_paths = [model_path for model_path in os.listdir(f'logs/{best_expt}') if 'model' in model_path]\n",
    "    best_model_path = sorted(best_model_paths, key=lambda x: x.split('_')[1])[-1]\n",
    "    return best_expt, best_model_path\n",
    "print_leaderboard_and_get_best_model_fast()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(8192, 512)\n",
       "    (wpe): Embedding(512, 512)\n",
       "    (h): ModuleList(\n",
       "      (0-15): 16 x Block(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=8192, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choosen_model = f'logs/{best_expt}/{best_model_path}'\n",
    "# choosen_model = f'logs/16l_16h_512d_quick/model_00323.pt'\n",
    "full_checkpoint = torch.load(choosen_model)\n",
    "config = full_checkpoint['config']\n",
    "m = GPT(config)\n",
    "\n",
    "def remove_orig_mod_prefix(state_dict):\n",
    "    return {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}\n",
    "\n",
    "m.load_state_dict(remove_orig_mod_prefix(full_checkpoint['model']))\n",
    "m.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838e83b152fe4567b4fad74e31d93c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/345 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce1838b5faa4404a5838672c2d94752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/324k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5678b9783c2a43d6ab223ce9c1fed2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/3.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc = AutoTokenizer.from_pretrained('activated-ai/tiny-stories-8k-tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0: Lily went to the park and saw a friendly dog. She said, \"Hello, doggy! Do you want to play with me?\" The dog barked happily. Lily and the dog played together for a long time.\n",
      "\n",
      "Suddenly, Lily's mom came to get her. She said, \"Lily, it's time to go home. What's in your hand?\" Lily showed her mom the penny and said, \"Look, Mommy! I found this in the park! Is this a penny?\" Her mom explained, \"Yes, that's a penny. It's a lot of money. We can use it to buy something at the store someday.\"\n",
      "\n",
      "Lily was happy to have met a friendly dog and a penny. She knew she would always remember the fun day at the park with her mom.\n",
      "sample 1: Lily went to the park and saw a friendly dog. The dog was very polite and didn't bark too loud. Lily thought the dog was nice and she admired his shiny collar. \n",
      "\n",
      "After playing in the park, Lily went home and told her mom about the dog she admired. Her mom was happy that Lily had a fun time and was polite to the dog. Lily also thought about the nice dog she saw at the park and felt happy. The end.\n",
      "sample 2: Lily went to the park and saw a friendly dog. The dog was happy to see her and wagged its tail. But when Lily tried to pet the dog, it bit her! Lily was scared and ran away. She ran so fast she didn't slow down. \n",
      "\n",
      "Lily ran for a long time until she got tired and lost. She looked around and saw her mommy calling her name. She ran to her mommy and told her what happened. Her mommy hugged her and said, \"I'm sorry, Lily. I know you wanted to be brave, but we have to be careful around dogs we don't know.\" From that day on, Lily was extra careful when running at the park.\n",
      "sample 3: Lily went to the park and saw a friendly dog. She observed the dog and said hello. The dog wagged its tail and licked her face. Lily giggled and petted the dog.\n",
      "\n",
      "Lily's finger felt better and she continued to play at the park. She realized that observing things could help her feel better when she was hurt. The end.\n"
     ]
    }
   ],
   "source": [
    "generate(m, enc, \"Lily went to the park and saw a friendly dog.\", 255, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
