{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt import GPTConfig, GPT, generate\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16l_16h_512d_quick                                 0.843 step 499 | val loss 3.4470 | byte loss 0.8426 | ds 90.2s\n"
     ]
    }
   ],
   "source": [
    "def print_leaderboard_and_get_best_model():\n",
    "    candidates = []\n",
    "    for expt in os.listdir('logs'):\n",
    "        if 'smoke' in expt:\n",
    "            continue\n",
    "        best_loss_for_expt = 10000\n",
    "        best_line = ''\n",
    "\n",
    "        for line in open(f'logs/{expt}/log.txt').readlines():\n",
    "\n",
    "            if 'val' in line and 'byte loss' in line:\n",
    "                def get_byte_loss():\n",
    "                    after_bl_str = line.split('byte loss')[1]            \n",
    "                    try:                \n",
    "                        byte_loss = float(after_bl_str.split(' ')[1])\n",
    "                    except ValueError as e:\n",
    "                        byte_loss = 100000\n",
    "                    return byte_loss\n",
    "                \n",
    "                byte_loss = get_byte_loss()\n",
    "\n",
    "                if byte_loss < best_loss_for_expt:\n",
    "                    best_loss_for_expt = byte_loss\n",
    "                    best_line = line\n",
    "        \n",
    "        if best_loss_for_expt < 10000:\n",
    "            candidates.append((best_loss_for_expt, expt, best_line))\n",
    "\n",
    "    candidates = sorted(candidates, key=lambda x: x[0])\n",
    "    for best_loss, expt, line in candidates:\n",
    "        print(expt.ljust(50), f'{best_loss:.3} {line.strip()}')\n",
    "\n",
    "    best_expt = candidates[0][1]\n",
    "    best_model_paths = [model_path for model_path in os.listdir(f'logs/{best_expt}') if 'model' in model_path]\n",
    "    best_model_path = sorted(best_model_paths, key=lambda x: x.split('_')[1])[-1]\n",
    "    return best_expt, best_model_path\n",
    "\n",
    "best_expt, best_model_path = print_leaderboard_and_get_best_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(8192, 512)\n",
       "    (wpe): Embedding(512, 512)\n",
       "    (h): ModuleList(\n",
       "      (0-15): 16 x Block(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=8192, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choosen_model = f'logs/{best_expt}/{best_model_path}'\n",
    "# choosen_model = f'logs/16l_16h_512d_quick/model_00323.pt'\n",
    "full_checkpoint = torch.load(choosen_model)\n",
    "config = full_checkpoint['config']\n",
    "m = GPT(config)\n",
    "\n",
    "def remove_orig_mod_prefix(state_dict):\n",
    "    return {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}\n",
    "\n",
    "m.load_state_dict(remove_orig_mod_prefix(full_checkpoint['model']))\n",
    "m.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = AutoTokenizer.from_pretrained('activated-ai/tiny-stories-8k-tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0: Lily went to the park and saw a friendly dog. They were playing in their eyes. They gave them a bird and then they played together in the man, but they played.\n",
      "\"Come, you for lunch!\" Mia said.\n",
      "\n",
      "\"Maybe we go outside and put on its room. We are a small bear.\n",
      "\n",
      "\n",
      "\"We were sad. But I wish we need to be careful,\" said. The deer said. The bear. The bird thought. They ran to the man and put the dog.\n",
      "Sara smiled. They also had to the sun back to say sorry for more. They gave the lady for the bear.\n",
      "\n",
      "\"I'm just there. They can play. And we are my ball out. Do you like the other,\" they feel glad they are not nice and good.\n",
      "sample 1: Lily went to the park and saw a friendly dog.\n",
      "She reached the tree and ran towards the tree started to his car.\n",
      "\"Look, you need?\" Mom said. \"Yes, and Ben with them and Ben.\n",
      "\n",
      "\n",
      "Her mom.\n",
      "\n",
      "Anna and Max saw Tom did that they were playing with their toys and took out. They looked at the blocks and the slide in their eyes. They pulled for them by the rain. They were hungry. They saw a lot of cars. They ran to the sand. They hugged them around and their own. They went near their hands. They went back to the best. They tried to their toys, not play. They were on the big. They played together. They were bad.\n",
      "\"No, mom. Or your sister's very nice.\" he said.\n",
      "Mom looked at their moms said they saw the park. They did not see the rain.\n",
      "Tom and Tom were happy and a big. She learned that even more things they were very nice and wanted to play. He tried to take up and angry.\n",
      "\n",
      "At the bench. It said. They said she had broken words and made him a big box.\n",
      "Spot and a loud noise. They said, they were not\n",
      "sample 2: Lily went to the park and saw a friendly dog.\n",
      "\n",
      "Soon, so many colorful animals were going to the day long day. They knew that they were very much.\n",
      "sample 3: Lily went to the park and saw a friendly dog.\n"
     ]
    }
   ],
   "source": [
    "generate(m, enc, \"Lily went to the park and saw a friendly dog.\", 255, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
