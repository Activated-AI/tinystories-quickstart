{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "import datasets\n",
    "import tqdm\n",
    "import torch\n",
    "import os\n",
    "\n",
    "vocab_size = 2**13\n",
    "vocab_name = '8k'\n",
    "output_dir = f'tiny-stories-{vocab_name}-eos'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spot. Spot saw the shiny car and said, \"Wow, Kitty, your car is so bright and clean!\" Kitty smiled and replied, \"Thank you, Spot. I polish it every day.\"\n",
      "\n",
      "After playing with the car, Kitty and Spot felt thirsty. They found a small pond with clear water. They drank the water and felt very happy. They played together all day and became best friends.\n",
      "Once upon a time, in a big forest, there lived a rhinoceros named Roxy. Roxy loved to climb. She climbed trees, rocks, and hills. One day, Roxy found an icy hill. She had never seen anything like it before. It was shiny and cold, and she wanted to climb it.\n",
      "\n",
      "Roxy tried to climb the icy hill, but it was very slippery. She tried again and again, but she kept falling down. Roxy was sad. She wanted to climb the icy hill so much. Then, she saw a little bird named Billy. Billy saw that Roxy was sad and asked, \"Why are you sad, Roxy?\"\n",
      "\n",
      "Roxy told Billy about the icy hill and how she couldn't climb it. Billy said, \"I have an idea! Let's find some big leaves to put under your feet. They will help you climb the icy hill.\" Roxy and Billy looked for big leaves and found some. Roxy put the leaves under her feet and tried to climb the icy hill again.\n",
      "\n",
      "This time, Roxy didn't slip. She climbed and climbed until she reached the top of the icy hill. Roxy was so happy! She and Billy played on the icy hill all day. From that day on, Roxy and Billy were the best of friends, and they climbed and played together all the time. And Roxy learned that with a little help from a friend, she could climb anything.\n",
      "Once upon a time, in a small yard, there was a small daisy. The daisy had a name. Her name was Daisy. Daisy was very small, but she was also very happy.\n",
      "\n",
      "One day, Daisy saw a dog. The dog was big and had a name too. His name was Max. Max liked to play in the yard. Daisy liked to watch Max play. Max and Daisy became friends.\n",
      "\n",
      "Every day, Max would come to the yard to play. Daisy would watch and smile. They were very happy together. And even though Daisy was small, she knew that she had a big friend in Max.\n",
      "2059\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "for t in ds['validation']['text'][:3]:\n",
    "    s += len(t)\n",
    "    print(t)\n",
    "    \n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tiny-stories-8k-eos/tokenizer-vocab.json',\n",
       " 'tiny-stories-8k-eos/tokenizer-merges.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tokenizers.ByteLevelBPETokenizer()\n",
    "\n",
    "def eos_iterator(ds):\n",
    "    for ex in ds:\n",
    "        yield ex + '[EOS]'\n",
    "\n",
    "t.train_from_iterator(eos_iterator(ds['train']['text']), vocab_size=vocab_size, min_frequency=2, special_tokens=['[EOS]'])\n",
    "t.save_model(output_dir, 'tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 2119719it [15:13, 2319.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size=8192 makes for 466747279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation: 21990it [00:07, 2901.71it/s]\n"
     ]
    }
   ],
   "source": [
    "for split in ['train', 'validation']:\n",
    "    output_tokens = []\n",
    "    for story in tqdm.tqdm(eos_iterator(ds[split]['text']), desc=split):    \n",
    "        output_tokens.extend(t.encode(story).ids)\n",
    "    if split == 'train':\n",
    "        print(f'{vocab_size=} makes for {len(output_tokens)}')\n",
    "    torch.save(torch.tensor(output_tokens, dtype=torch.int16), f'{output_dir}/{split}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train data size (chars) 1899973203\n",
    "\n",
    "# 4k vocab ->  483664338 tokens -> 3.928 chars per token .  approx 1030 Mtok/sec with a 10.8 M model\n",
    "# 8k vocab ->  464614341 tokens -> 4.089 chars per token .  approx 950 Mtok/sec with a 11.6M model \n",
    "# 16k vocab -> 460959001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
